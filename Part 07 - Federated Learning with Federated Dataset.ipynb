{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 - Federated Learning with FederatedDataset\n",
    "\n",
    "Here we introduce a new tool for using federated datasets. We have created a `FederatedDataset` class which is intended to be used like the PyTorch Dataset class, and is given to a federated data loader `FederatedDataLoader` which will iterate on it in a federated fashion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "import syft as sy\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "# The local worker\n",
    "me = hook.local_worker\n",
    "me.is_client_worker = False\n",
    "# The remote workers\n",
    "bob = DataCentricFLClient(hook, \"http://18.220.216.78:5001/\")\n",
    "alice = DataCentricFLClient(hook, \"http://18.220.216.78:5002/\")\n",
    "# The crypto provider\n",
    "crypto_provider = DataCentricFLClient(hook, \"http://18.220.216.78:5003/\")\n",
    "my_grid = sy.PrivateGridNetwork(bob,alice,crypto_provider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kim = DataCentricFLClient(hook, \"http://18.220.216.78:5001/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.7990e+01, 1.0380e+01, 1.2280e+02,  ..., 2.6540e-01, 4.6010e-01,\n",
       "          1.1890e-01],\n",
       "         [2.0570e+01, 1.7770e+01, 1.3290e+02,  ..., 1.8600e-01, 2.7500e-01,\n",
       "          8.9020e-02],\n",
       "         [1.9690e+01, 2.1250e+01, 1.3000e+02,  ..., 2.4300e-01, 3.6130e-01,\n",
       "          8.7580e-02],\n",
       "         ...,\n",
       "         [1.6600e+01, 2.8080e+01, 1.0830e+02,  ..., 1.4180e-01, 2.2180e-01,\n",
       "          7.8200e-02],\n",
       "         [2.0600e+01, 2.9330e+01, 1.4010e+02,  ..., 2.6500e-01, 4.0870e-01,\n",
       "          1.2400e-01],\n",
       "         [7.7600e+00, 2.4540e+01, 4.7920e+01,  ..., 0.0000e+00, 2.8710e-01,\n",
       "          7.0390e-02]])\n",
       " \tTags: _breast_cancer_dataset: .. #data \n",
       " \tDescription: .. _breast_cancer_dataset:...\n",
       " \tShape: torch.Size([569, 30]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "         0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
       "         1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "         1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
       " \tTags: _breast_cancer_dataset: #target .. \n",
       " \tDescription: .. _breast_cancer_dataset:...\n",
       " \tShape: torch.Size([569]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_diabetes\n",
    "def load_sklearn(func, *tags):\n",
    "        dataset = func()\n",
    "        data = (\n",
    "            torch.tensor(dataset[\"data\"])\n",
    "            .float()\n",
    "            .tag(*(list(tags) + [\"#data\"] + dataset[\"DESCR\"].split(\"\\n\")[0].lower().split(\" \")))\n",
    "            .describe(dataset[\"DESCR\"])\n",
    "        )\n",
    "        target = (\n",
    "            torch.tensor(dataset[\"target\"])\n",
    "            .float()\n",
    "            .tag(\n",
    "                *(list(tags) + [\"#target\"] + dataset[\"DESCR\"].split(\"\\n\")[0].lower().split(\" \"))\n",
    "            )\n",
    "            .describe(dataset[\"DESCR\"])\n",
    "        )\n",
    "        me.register_obj(data)\n",
    "        me.register_obj(target)\n",
    "\n",
    "        return data, target\n",
    "data,targer=load_sklearn(load_boston, *[\"#boston\", \"#housing\", \"#boston_housing\"])\n",
    "load_sklearn(load_diabetes, *[\"#diabetes\"])\n",
    "load_sklearn(load_breast_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:40419911065 -> Bob:17862325987]\n",
       "\tTags: #housing #boston_housing _boston_dataset: .. #data #boston \n",
       "\tShape: torch.Size([506, 13])\n",
       "\tDescription: .. _boston_dataset:..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.send(kim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = grid.search(\"#boston\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bob': [(Wrapper)>[PointerTensor | me:93065209375 -> bob:82502429288]\n",
       "  \tTags: #housing #boston #target _boston_dataset: #boston_housing .. \n",
       "  \tShape: torch.Size([506])\n",
       "  \tDescription: .. _boston_dataset:...,\n",
       "  (Wrapper)>[PointerTensor | me:29805147575 -> bob:55315003130]\n",
       "  \tTags: #data #housing #boston _boston_dataset: #boston_housing .. \n",
       "  \tShape: torch.Size([506, 13])\n",
       "  \tDescription: .. _boston_dataset:...],\n",
       " 'sam': [(Wrapper)>[PointerTensor | me:25764275142 -> sam:7544631372]\n",
       "  \tTags: #housing #boston #target _boston_dataset: #boston_housing .. \n",
       "  \tShape: torch.Size([506])\n",
       "  \tDescription: .. _boston_dataset:...,\n",
       "  (Wrapper)>[PointerTensor | me:57892890242 -> sam:18565750255]\n",
       "  \tTags: #data #boston #housing _boston_dataset: #boston_housing .. \n",
       "  \tShape: torch.Size([506, 13])\n",
       "  \tDescription: .. _boston_dataset:...]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr = data['bob'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#boston', '#boston_housing', '#housing', '#target', '..', '_boston_dataset:'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptr.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptr.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = [[id,val[0].description, (list(val[0].shape)), val[0].tags] for id,val in data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bob',\n",
       "  \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       "  [506],\n",
       "  {'#boston',\n",
       "   '#boston_housing',\n",
       "   '#housing',\n",
       "   '#target',\n",
       "   '..',\n",
       "   '_boston_dataset:'}],\n",
       " ['sam',\n",
       "  \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       "  [506],\n",
       "  {'#boston',\n",
       "   '#boston_housing',\n",
       "   '#housing',\n",
       "   '#target',\n",
       "   '..',\n",
       "   '_boston_dataset:'}]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ws://localhost:5005'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me._known_workers['bob'].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Location                                        Description   Tags  \\\n",
      "0      bob  .. _boston_dataset:\\n\\nBoston house prices dat...  [506]   \n",
      "1      sam  .. _boston_dataset:\\n\\nBoston house prices dat...  [506]   \n",
      "\n",
      "                                                Size  \n",
      "0  {#housing, #boston_housing, _boston_dataset:, ...  \n",
      "1  {#housing, #boston_housing, _boston_dataset:, ...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(nd,columns=['Location','Description','Tags','Size'])\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then search for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd['bob']= ptr.description\n",
    "nd['sam'] = data['sam'][0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Series(nd, name='Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bob</th>\n",
       "      <th>sam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.. _boston_dataset:\\n\\nBoston house prices dat...</td>\n",
       "      <td>.. _boston_dataset:\\n\\nBoston house prices dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[506]</td>\n",
       "      <td>[506]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{#housing, #boston_housing, _boston_dataset:, ...</td>\n",
       "      <td>{#housing, #boston_housing, _boston_dataset:, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 bob  \\\n",
       "0  .. _boston_dataset:\\n\\nBoston house prices dat...   \n",
       "1                                              [506]   \n",
       "2  {#housing, #boston_housing, _boston_dataset:, ...   \n",
       "\n",
       "                                                 sam  \n",
       "0  .. _boston_dataset:\\n\\nBoston house prices dat...  \n",
       "1                                              [506]  \n",
       "2  {#housing, #boston_housing, _boston_dataset:, ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.name = 'location'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bob</th>\n",
       "      <td>[.. _boston_dataset:\\n\\nBoston house prices da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam</th>\n",
       "      <td>[.. _boston_dataset:\\n\\nBoston house prices da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Description\n",
       "location                                                   \n",
       "bob       [.. _boston_dataset:\\n\\nBoston house prices da...\n",
       "sam       [.. _boston_dataset:\\n\\nBoston house prices da..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bob': [(Wrapper)>[PointerTensor | me:53394938518 -> bob:28521435288]\n",
       "  \tTags: #data .. _breast_cancer_dataset: \n",
       "  \tShape: torch.Size([569, 30])\n",
       "  \tDescription: .. _breast_cancer_dataset:...,\n",
       "  (Wrapper)>[PointerTensor | me:11550955434 -> bob:87195006662]\n",
       "  \tTags: #diabetes #data .. _diabetes_dataset: \n",
       "  \tShape: torch.Size([442, 10])\n",
       "  \tDescription: .. _diabetes_dataset:...,\n",
       "  (Wrapper)>[PointerTensor | me:93695107669 -> bob:96238330179]\n",
       "  \tTags: #boston_housing #data _boston_dataset: #housing #boston .. \n",
       "  \tShape: torch.Size([506, 13])\n",
       "  \tDescription: .. _boston_dataset:...],\n",
       " 'alice': [(Wrapper)>[PointerTensor | me:61477233258 -> alice:61146921641]\n",
       "  \tTags: #boston_housing #data _boston_dataset: #housing #boston .. \n",
       "  \tShape: torch.Size([506, 13])\n",
       "  \tDescription: .. _boston_dataset:...,\n",
       "  (Wrapper)>[PointerTensor | me:10648270773 -> alice:55156661739]\n",
       "  \tTags: #diabetes #data .. _diabetes_dataset: \n",
       "  \tShape: torch.Size([442, 10])\n",
       "  \tDescription: .. _diabetes_dataset:...,\n",
       "  (Wrapper)>[PointerTensor | me:17713585958 -> alice:18527685518]\n",
       "  \tTags: #data .. _breast_cancer_dataset: \n",
       "  \tShape: torch.Size([569, 30])\n",
       "  \tDescription: .. _breast_cancer_dataset:...],\n",
       " 'sam': [(Wrapper)>[PointerTensor | me:11451140388 -> sam:34430670552]\n",
       "  \tTags: #diabetes #data .. _diabetes_dataset: \n",
       "  \tShape: torch.Size([442, 10])\n",
       "  \tDescription: .. _diabetes_dataset:...,\n",
       "  (Wrapper)>[PointerTensor | me:1923776301 -> sam:45460266713]\n",
       "  \tTags: #boston_housing #data _boston_dataset: #housing #boston .. \n",
       "  \tShape: torch.Size([506, 13])\n",
       "  \tDescription: .. _boston_dataset:...,\n",
       "  (Wrapper)>[PointerTensor | me:31508758102 -> sam:62129738939]\n",
       "  \tTags: #data .. _breast_cancer_dataset: \n",
       "  \tShape: torch.Size([569, 30])\n",
       "  \tDescription: .. _breast_cancer_dataset:...]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.search( \"#data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data = grid.search(\"#boston\", \"#data\")\n",
    "boston_target = grid.search(\"#boston\", \"#target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bob': [(Wrapper)>[PointerTensor | me:36808794346 -> bob:96238330179]\n",
       "  \tTags: #boston_housing #data _boston_dataset: #housing #boston .. \n",
       "  \tShape: torch.Size([506, 13])\n",
       "  \tDescription: .. _boston_dataset:...],\n",
       " 'alice': [(Wrapper)>[PointerTensor | me:64223157257 -> alice:61146921641]\n",
       "  \tTags: #boston_housing #data _boston_dataset: #housing #boston .. \n",
       "  \tShape: torch.Size([506, 13])\n",
       "  \tDescription: .. _boston_dataset:...],\n",
       " 'sam': [(Wrapper)>[PointerTensor | me:6827837124 -> sam:45460266713]\n",
       "  \tTags: #boston_housing #data _boston_dataset: #housing #boston .. \n",
       "  \tShape: torch.Size([506, 13])\n",
       "  \tDescription: .. _boston_dataset:...]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a model and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = boston_data['alice'][0].shape[1]\n",
    "n_targets = 1\n",
    "\n",
    "model = th.nn.Linear(n_features, n_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we cast the data fetched in a `FederatedDataset`. See the workers which hold part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bob', 'alice', 'sam']\n"
     ]
    }
   ],
   "source": [
    "# Cast the result in BaseDatasets\n",
    "datasets = []\n",
    "for worker in boston_data.keys():\n",
    "    dataset = sy.BaseDataset(boston_data[worker][0], boston_target[worker][0])\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "dataset = sy.FederatedDataset(datasets)\n",
    "print(dataset.workers)\n",
    "optimizers = {}\n",
    "for worker in dataset.workers:\n",
    "    optimizers[worker] = th.optim.Adam(params=model.parameters(),lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put it in a `FederatedDataLoader` and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = sy.FederatedDataLoader(dataset, batch_size=3, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we iterate over epochs. You can see how similar this is compared to pure and local PyTorch training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/506 (0%)]\tBatch loss: 1142.767212\n",
      "Train Epoch: 1 [8/506 (2%)]\tBatch loss: 711.976562\n",
      "Train Epoch: 1 [16/506 (3%)]\tBatch loss: 737.107727\n",
      "Train Epoch: 1 [24/506 (5%)]\tBatch loss: 164.395462\n",
      "Train Epoch: 1 [32/506 (6%)]\tBatch loss: 274.209290\n",
      "Train Epoch: 1 [40/506 (8%)]\tBatch loss: 1304.583130\n",
      "Train Epoch: 1 [48/506 (9%)]\tBatch loss: 856.446960\n",
      "Train Epoch: 1 [56/506 (11%)]\tBatch loss: 53.861355\n",
      "Train Epoch: 1 [64/506 (13%)]\tBatch loss: 361.134155\n",
      "Train Epoch: 1 [72/506 (14%)]\tBatch loss: 569.727905\n",
      "Train Epoch: 1 [80/506 (16%)]\tBatch loss: 421.269928\n",
      "Train Epoch: 1 [88/506 (17%)]\tBatch loss: 203.238724\n",
      "Train Epoch: 1 [96/506 (19%)]\tBatch loss: 83.544807\n",
      "Train Epoch: 1 [104/506 (21%)]\tBatch loss: 501.499237\n",
      "Train Epoch: 1 [112/506 (22%)]\tBatch loss: 270.610504\n",
      "Train Epoch: 1 [120/506 (24%)]\tBatch loss: 2643.436523\n",
      "Train Epoch: 1 [128/506 (25%)]\tBatch loss: 312.704529\n",
      "Train Epoch: 1 [136/506 (27%)]\tBatch loss: 980.239563\n",
      "Train Epoch: 1 [144/506 (28%)]\tBatch loss: 63.504349\n",
      "Train Epoch: 1 [152/506 (30%)]\tBatch loss: 275.837677\n",
      "Train Epoch: 1 [160/506 (32%)]\tBatch loss: 68.200775\n",
      "Train Epoch: 1 [168/506 (33%)]\tBatch loss: 115.070381\n",
      "Train Epoch: 1 [176/506 (35%)]\tBatch loss: 176.779724\n",
      "Train Epoch: 1 [184/506 (36%)]\tBatch loss: 330.648712\n",
      "Train Epoch: 1 [192/506 (38%)]\tBatch loss: 31.223417\n",
      "Train Epoch: 1 [200/506 (40%)]\tBatch loss: 28.009399\n",
      "Train Epoch: 1 [208/506 (41%)]\tBatch loss: 10.939261\n",
      "Train Epoch: 1 [216/506 (43%)]\tBatch loss: 3.835716\n",
      "Train Epoch: 1 [224/506 (44%)]\tBatch loss: 192.751587\n",
      "Train Epoch: 1 [232/506 (46%)]\tBatch loss: 174.528488\n",
      "Train Epoch: 1 [240/506 (47%)]\tBatch loss: 25.354095\n",
      "Train Epoch: 1 [248/506 (49%)]\tBatch loss: 3.864616\n",
      "Train Epoch: 1 [256/506 (51%)]\tBatch loss: 102.869080\n",
      "Train Epoch: 1 [264/506 (52%)]\tBatch loss: 186.399124\n",
      "Train Epoch: 1 [272/506 (54%)]\tBatch loss: 75.950317\n",
      "Train Epoch: 1 [280/506 (55%)]\tBatch loss: 148.165024\n",
      "Train Epoch: 1 [288/506 (57%)]\tBatch loss: 581.127502\n",
      "Train Epoch: 1 [296/506 (58%)]\tBatch loss: 83.286049\n",
      "Train Epoch: 1 [304/506 (60%)]\tBatch loss: 303.008392\n",
      "Train Epoch: 1 [312/506 (62%)]\tBatch loss: 161.320557\n",
      "Train Epoch: 1 [320/506 (63%)]\tBatch loss: 132.646210\n",
      "Train Epoch: 1 [328/506 (65%)]\tBatch loss: 166.790543\n",
      "Train Epoch: 1 [336/506 (66%)]\tBatch loss: 83.640144\n",
      "Train Epoch: 1 [344/506 (68%)]\tBatch loss: 82.742470\n",
      "Train Epoch: 1 [352/506 (70%)]\tBatch loss: 289.116974\n",
      "Train Epoch: 1 [360/506 (71%)]\tBatch loss: 146.118225\n",
      "Train Epoch: 1 [368/506 (73%)]\tBatch loss: 3.267447\n",
      "Train Epoch: 1 [376/506 (74%)]\tBatch loss: 12.515526\n",
      "Train Epoch: 1 [384/506 (76%)]\tBatch loss: 3.027637\n",
      "Train Epoch: 1 [392/506 (77%)]\tBatch loss: 229.626297\n",
      "Train Epoch: 1 [400/506 (79%)]\tBatch loss: 111.730507\n",
      "Train Epoch: 1 [408/506 (81%)]\tBatch loss: 133.614761\n",
      "Train Epoch: 1 [416/506 (82%)]\tBatch loss: 7.218603\n",
      "Train Epoch: 1 [424/506 (84%)]\tBatch loss: 35.056446\n",
      "Train Epoch: 1 [432/506 (85%)]\tBatch loss: 86.790009\n",
      "Train Epoch: 1 [440/506 (87%)]\tBatch loss: 20.679306\n",
      "Train Epoch: 1 [448/506 (89%)]\tBatch loss: 7.396908\n",
      "Train Epoch: 1 [456/506 (90%)]\tBatch loss: 39.629940\n",
      "Train Epoch: 1 [464/506 (92%)]\tBatch loss: 424.999054\n",
      "Train Epoch: 1 [472/506 (93%)]\tBatch loss: 255.379272\n",
      "Train Epoch: 1 [480/506 (95%)]\tBatch loss: 243.144730\n",
      "Train Epoch: 1 [488/506 (96%)]\tBatch loss: 196.369858\n",
      "Train Epoch: 1 [496/506 (98%)]\tBatch loss: 216.243164\n",
      "Train Epoch: 1 [504/506 (100%)]\tBatch loss: 46.658325\n",
      "Total loss 160505.12345564365\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    loss_accum = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        model.send(data.location)\n",
    "        \n",
    "        optimizer = optimizers[data.location.id]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = ((pred.view(-1) - target)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.get()\n",
    "        loss = loss.get()\n",
    "        \n",
    "        loss_accum += float(loss)\n",
    "        \n",
    "        if batch_idx % 8 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBatch loss: {:.6f}'.format(\n",
    "                epoch, batch_idx, len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))            \n",
    "            \n",
    "    print('Total loss', loss_accum)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
